{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a38a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import rtdl\n",
    "from catboost.datasets import epsilon\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import zero\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67db2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123456"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed:\n",
    "zero.improve_reproducibility(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14784a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_train, eps_test = epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51487c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_train = pd.read_csv(\"data/epsilon/eps_train.csv\")\n",
    "eps_test = pd.read_csv(\"data/epsilon/eps_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1939dac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>-0.002186</td>\n",
       "      <td>-0.014590</td>\n",
       "      <td>0.015631</td>\n",
       "      <td>-0.032606</td>\n",
       "      <td>-0.004455</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029023</td>\n",
       "      <td>0.028153</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>-0.048453</td>\n",
       "      <td>-0.030330</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.022238</td>\n",
       "      <td>-0.009459</td>\n",
       "      <td>0.027544</td>\n",
       "      <td>-0.026216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.033820</td>\n",
       "      <td>-0.048836</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>-0.028718</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>-0.006827</td>\n",
       "      <td>0.053082</td>\n",
       "      <td>-0.016931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>-0.017588</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.037386</td>\n",
       "      <td>-0.002207</td>\n",
       "      <td>0.023466</td>\n",
       "      <td>0.023459</td>\n",
       "      <td>0.036497</td>\n",
       "      <td>0.033899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>-0.042784</td>\n",
       "      <td>-0.004416</td>\n",
       "      <td>-0.005692</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>-0.025873</td>\n",
       "      <td>0.031471</td>\n",
       "      <td>0.059522</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020841</td>\n",
       "      <td>-0.030902</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>-0.017727</td>\n",
       "      <td>-0.011851</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>-0.002806</td>\n",
       "      <td>-0.004059</td>\n",
       "      <td>0.024565</td>\n",
       "      <td>-0.001050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.009080</td>\n",
       "      <td>0.017605</td>\n",
       "      <td>-0.009870</td>\n",
       "      <td>0.007386</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>-0.042682</td>\n",
       "      <td>-0.004471</td>\n",
       "      <td>0.035229</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025915</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.022496</td>\n",
       "      <td>-0.005170</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>-0.036478</td>\n",
       "      <td>-0.036575</td>\n",
       "      <td>-0.002760</td>\n",
       "      <td>-0.021496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.011689</td>\n",
       "      <td>-0.021413</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>-0.009324</td>\n",
       "      <td>-0.023587</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>-0.020042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>-0.016919</td>\n",
       "      <td>0.047534</td>\n",
       "      <td>-0.004458</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.036077</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>0.008140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.022291</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>0.020427</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.030112</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024601</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.016945</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.037605</td>\n",
       "      <td>0.028264</td>\n",
       "      <td>-0.001602</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>-0.003627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.011844</td>\n",
       "      <td>0.020148</td>\n",
       "      <td>-0.026867</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>-0.011501</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>-0.037498</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>-0.025452</td>\n",
       "      <td>-0.013585</td>\n",
       "      <td>-0.004808</td>\n",
       "      <td>-0.020536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026138</td>\n",
       "      <td>0.014778</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>-0.026422</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.048577</td>\n",
       "      <td>0.025201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026015</td>\n",
       "      <td>-0.003136</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>-0.011271</td>\n",
       "      <td>0.040341</td>\n",
       "      <td>-0.020855</td>\n",
       "      <td>-0.020545</td>\n",
       "      <td>-0.022856</td>\n",
       "      <td>0.049474</td>\n",
       "      <td>-0.010935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.031459</td>\n",
       "      <td>-0.035493</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>-0.030789</td>\n",
       "      <td>-0.013167</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.035517</td>\n",
       "      <td>0.007557</td>\n",
       "      <td>-0.012632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031911</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>0.015954</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>-0.001581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>0.033481</td>\n",
       "      <td>-0.018320</td>\n",
       "      <td>-0.011766</td>\n",
       "      <td>0.036155</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.011863</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041014</td>\n",
       "      <td>-0.020519</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>-0.002157</td>\n",
       "      <td>-0.011022</td>\n",
       "      <td>-0.019959</td>\n",
       "      <td>-0.032119</td>\n",
       "      <td>-0.042401</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>-0.009653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6  \\\n",
       "0     -1.0  0.005439  0.013331 -0.002186 -0.014590  0.015631 -0.032606   \n",
       "1      1.0  0.001442  0.033820 -0.048836  0.000652 -0.028718  0.013421   \n",
       "2      1.0  0.004597 -0.042784 -0.004416 -0.005692  0.000731 -0.025873   \n",
       "3     -1.0 -0.009080  0.017605 -0.009870  0.007386  0.021338 -0.042682   \n",
       "4      1.0 -0.011689 -0.021413  0.012358 -0.012036 -0.009324 -0.023587   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "99995 -1.0  0.002185  0.004446  0.022291  0.020920  0.020427  0.012598   \n",
       "99996  1.0 -0.011844  0.020148 -0.026867  0.003328  0.027158  0.005969   \n",
       "99997  1.0  0.026138  0.014778  0.001464 -0.026422  0.009453 -0.011432   \n",
       "99998  1.0 -0.031459 -0.035493  0.003074 -0.030789 -0.013167  0.002857   \n",
       "99999 -1.0 -0.024012  0.033481 -0.018320 -0.011766  0.036155 -0.000876   \n",
       "\n",
       "              7         8         9  ...      1991      1992      1993  \\\n",
       "0     -0.004455  0.013611  0.024088  ...  0.029023  0.028153 -0.001714   \n",
       "1     -0.006827  0.053082 -0.016931  ... -0.016411  0.005543 -0.017588   \n",
       "2      0.031471  0.059522  0.003261  ... -0.020841 -0.030902  0.005387   \n",
       "3     -0.004471  0.035229  0.037935  ... -0.025915  0.005119  0.022496   \n",
       "4      0.007309  0.034027 -0.020042  ...  0.006731  0.011447 -0.015999   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "99995  0.030112  0.011090  0.001913  ... -0.024601  0.006985  0.016945   \n",
       "99996  0.017370 -0.011501  0.029321  ...  0.026503 -0.025448 -0.037498   \n",
       "99997  0.017415  0.048577  0.025201  ...  0.026015 -0.003136  0.020917   \n",
       "99998  0.035517  0.007557 -0.012632  ...  0.031911  0.028818  0.015954   \n",
       "99999  0.005860  0.011863  0.029360  ... -0.041014 -0.020519 -0.003613   \n",
       "\n",
       "           1994      1995      1996      1997      1998      1999      2000  \n",
       "0     -0.048453 -0.030330 -0.006301 -0.022238 -0.009459  0.027544 -0.026216  \n",
       "1      0.005169  0.037386 -0.002207  0.023466  0.023459  0.036497  0.033899  \n",
       "2     -0.017727 -0.011851  0.007834 -0.002806 -0.004059  0.024565 -0.001050  \n",
       "3     -0.005170 -0.023424 -0.026319 -0.036478 -0.036575 -0.002760 -0.021496  \n",
       "4     -0.016919  0.047534 -0.004458  0.013541  0.036077 -0.004313  0.008140  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "99995  0.007586  0.037605  0.028264 -0.001602  0.001870  0.006102 -0.003627  \n",
       "99996 -0.027911  0.010793 -0.004598 -0.025452 -0.013585 -0.004808 -0.020536  \n",
       "99997 -0.011271  0.040341 -0.020855 -0.020545 -0.022856  0.049474 -0.010935  \n",
       "99998 -0.005749  0.003205  0.018634  0.010508  0.001254  0.010319 -0.001581  \n",
       "99999 -0.002157 -0.011022 -0.019959 -0.032119 -0.042401  0.005638 -0.009653  \n",
       "\n",
       "[100000 rows x 2001 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99fbdbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7976f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "task_type = \"binclass\"\n",
    "\n",
    "X_tr = eps_train.iloc[:, 1:].astype('float32')\n",
    "y_tr = eps_train.iloc[:, 0].astype('float32' if task_type == 'regression' else 'int64')\n",
    "#\n",
    "X_te = eps_test.iloc[:, 1:].astype('float32')\n",
    "y_te = eps_test.iloc[:, 0].astype('float32' if task_type == 'regression' else 'int64')\n",
    "if task_type != 'regression':\n",
    "    y_tr = LabelEncoder().fit_transform(y_tr).astype('int64')\n",
    "    y_te = LabelEncoder().fit_transform(y_te).astype('int64')\n",
    "n_classes = int(max(y_tr)) + 1 if task_type == 'multiclass' else None\n",
    "\n",
    "X = {}\n",
    "y = {}\n",
    "\n",
    "X['test'] = X_te\n",
    "y['test'] = y_te\n",
    "\n",
    "X['train'], X['val'], y['train'], y['val'] = train_test_split(X_tr, y_tr, train_size=0.8, stratify=y_tr)\n",
    "\n",
    "X = {\n",
    "    k: torch.tensor(v.to_numpy(), device=device)\n",
    "    for k, v in X.items()\n",
    "}\n",
    "y = {k: torch.tensor(v, device=device) for k, v in y.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de5f9d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = n_classes or 1\n",
    "\n",
    "# model = rtdl.MLP.make_baseline(\n",
    "#     d_in=X_tr.shape[1],\n",
    "#     d_layers=[1024, 512, 256],\n",
    "#     dropout=0.1,\n",
    "#     d_out=d_out,\n",
    "# )\n",
    "# lr = 0.001\n",
    "# weight_decay = 0.0\n",
    "\n",
    "model = rtdl.ResNet.make_baseline(\n",
    "    d_in=X_tr.shape[1],\n",
    "    d_main=128,\n",
    "    d_hidden=256,\n",
    "    dropout_first=0.2,\n",
    "    dropout_second=0.0,\n",
    "    n_blocks=2,\n",
    "    d_out=d_out,\n",
    ")\n",
    "lr = 0.001\n",
    "weight_decay = 0.0\n",
    "\n",
    "# model = rtdl.FTTransformer.make_default(\n",
    "#     n_num_features=X_all.shape[1],\n",
    "#     cat_cardinalities=None,\n",
    "#     last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "#     d_out=d_out,\n",
    "# )\n",
    "\n",
    "model.to(device)\n",
    "optimizer = (\n",
    "    model.make_default_optimizer()\n",
    "    if isinstance(model, rtdl.FTTransformer)\n",
    "    else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    ")\n",
    "loss_fn = (\n",
    "    F.binary_cross_entropy_with_logits\n",
    "    if task_type == 'binclass'\n",
    "    else F.cross_entropy\n",
    "    if task_type == 'multiclass'\n",
    "    else F.mse_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17a93392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (first_layer): Linear(in_features=2000, out_features=128, bias=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (linear_first): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (activation): ReLU()\n",
       "      (dropout_first): Dropout(p=0.2, inplace=False)\n",
       "      (linear_second): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (dropout_second): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (linear_first): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (activation): ReLU()\n",
       "      (dropout_first): Dropout(p=0.2, inplace=False)\n",
       "      (linear_second): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (dropout_second): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (head): Head(\n",
       "    (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU()\n",
       "    (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082863ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score before training: 0.4995\n"
     ]
    }
   ],
   "source": [
    "def apply_model(x_num, x_cat=None):\n",
    "    if isinstance(model, rtdl.FTTransformer):\n",
    "        return model(x_num, x_cat)\n",
    "    elif isinstance(model, (rtdl.MLP, rtdl.ResNet)):\n",
    "        assert x_cat is None\n",
    "        return model(x_num)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f'Looks like you are using a custom model: {type(model)}.'\n",
    "            ' Then you have to implement this branch first.'\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(part):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    for batch in zero.iter_batches(X[part], 1024):\n",
    "        prediction.append(apply_model(batch))\n",
    "    prediction = torch.cat(prediction).squeeze(1).cpu().numpy()\n",
    "    target = y[part].cpu().numpy()\n",
    "\n",
    "    if task_type == 'binclass':\n",
    "        prediction = np.round(scipy.special.expit(prediction))\n",
    "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
    "    elif task_type == 'multiclass':\n",
    "        prediction = prediction.argmax(1)\n",
    "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
    "    else:\n",
    "        assert task_type == 'regression'\n",
    "        score = sklearn.metrics.mean_squared_error(target, prediction) ** 0.5 * y_std\n",
    "    return score\n",
    "\n",
    "\n",
    "# Create a dataloader for batches of indices\n",
    "# Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n",
    "batch_size = 256\n",
    "train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device=device)\n",
    "\n",
    "# Create a progress tracker for early stopping\n",
    "# Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n",
    "progress = zero.ProgressTracker(patience=20)\n",
    "\n",
    "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58e0fc44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch) 1 (batch) 0 (loss) 0.7133\n",
      "(epoch) 1 (batch) 250 (loss) 0.2917\n",
      "(epoch) 1 (batch) 500 (loss) 0.2903\n",
      "(epoch) 1 (batch) 750 (loss) 0.2964\n",
      "(epoch) 1 (batch) 1000 (loss) 0.2531\n",
      "Epoch 001 | Validation score: 0.8884 | Test score: 0.8888 <<< BEST VALIDATION EPOCH\n",
      "(epoch) 2 (batch) 0 (loss) 0.2174\n",
      "(epoch) 2 (batch) 250 (loss) 0.2856\n",
      "(epoch) 2 (batch) 500 (loss) 0.2721\n",
      "(epoch) 2 (batch) 750 (loss) 0.2760\n",
      "(epoch) 2 (batch) 1000 (loss) 0.2407\n",
      "Epoch 002 | Validation score: 0.8879 | Test score: 0.8872\n",
      "(epoch) 3 (batch) 0 (loss) 0.2064\n",
      "(epoch) 3 (batch) 250 (loss) 0.2788\n",
      "(epoch) 3 (batch) 500 (loss) 0.2601\n",
      "(epoch) 3 (batch) 750 (loss) 0.2725\n",
      "(epoch) 3 (batch) 1000 (loss) 0.2412\n",
      "Epoch 003 | Validation score: 0.8927 | Test score: 0.8919 <<< BEST VALIDATION EPOCH\n",
      "(epoch) 4 (batch) 0 (loss) 0.2020\n",
      "(epoch) 4 (batch) 250 (loss) 0.2498\n",
      "(epoch) 4 (batch) 500 (loss) 0.2726\n",
      "(epoch) 4 (batch) 750 (loss) 0.2681\n",
      "(epoch) 4 (batch) 1000 (loss) 0.2418\n",
      "Epoch 004 | Validation score: 0.8938 | Test score: 0.8935 <<< BEST VALIDATION EPOCH\n",
      "(epoch) 5 (batch) 0 (loss) 0.1951\n",
      "(epoch) 5 (batch) 250 (loss) 0.2397\n",
      "(epoch) 5 (batch) 500 (loss) 0.2720\n",
      "(epoch) 5 (batch) 750 (loss) 0.2651\n",
      "(epoch) 5 (batch) 1000 (loss) 0.2362\n",
      "Epoch 005 | Validation score: 0.8935 | Test score: 0.8931\n",
      "(epoch) 6 (batch) 0 (loss) 0.1926\n",
      "(epoch) 6 (batch) 250 (loss) 0.2491\n",
      "(epoch) 6 (batch) 500 (loss) 0.2575\n",
      "(epoch) 6 (batch) 750 (loss) 0.2585\n",
      "(epoch) 6 (batch) 1000 (loss) 0.2422\n",
      "Epoch 006 | Validation score: 0.8913 | Test score: 0.8914\n",
      "(epoch) 7 (batch) 0 (loss) 0.1845\n",
      "(epoch) 7 (batch) 250 (loss) 0.2586\n",
      "(epoch) 7 (batch) 500 (loss) 0.2548\n",
      "(epoch) 7 (batch) 750 (loss) 0.2635\n",
      "(epoch) 7 (batch) 1000 (loss) 0.2398\n",
      "Epoch 007 | Validation score: 0.8922 | Test score: 0.8929\n",
      "(epoch) 8 (batch) 0 (loss) 0.1920\n",
      "(epoch) 8 (batch) 250 (loss) 0.2520\n",
      "(epoch) 8 (batch) 500 (loss) 0.2498\n",
      "(epoch) 8 (batch) 750 (loss) 0.2493\n",
      "(epoch) 8 (batch) 1000 (loss) 0.2290\n",
      "Epoch 008 | Validation score: 0.8925 | Test score: 0.8924\n",
      "(epoch) 9 (batch) 0 (loss) 0.1949\n",
      "(epoch) 9 (batch) 250 (loss) 0.2456\n",
      "(epoch) 9 (batch) 500 (loss) 0.2456\n",
      "(epoch) 9 (batch) 750 (loss) 0.2449\n",
      "(epoch) 9 (batch) 1000 (loss) 0.2094\n",
      "Epoch 009 | Validation score: 0.8928 | Test score: 0.8918\n",
      "(epoch) 10 (batch) 0 (loss) 0.1892\n",
      "(epoch) 10 (batch) 250 (loss) 0.2445\n",
      "(epoch) 10 (batch) 500 (loss) 0.2440\n",
      "(epoch) 10 (batch) 750 (loss) 0.2465\n",
      "(epoch) 10 (batch) 1000 (loss) 0.2223\n",
      "Epoch 010 | Validation score: 0.8925 | Test score: 0.8922\n",
      "(epoch) 11 (batch) 0 (loss) 0.1897\n",
      "(epoch) 11 (batch) 250 (loss) 0.2425\n",
      "(epoch) 11 (batch) 500 (loss) 0.2468\n",
      "(epoch) 11 (batch) 750 (loss) 0.2413\n",
      "(epoch) 11 (batch) 1000 (loss) 0.2291\n",
      "Epoch 011 | Validation score: 0.8907 | Test score: 0.8903\n",
      "(epoch) 12 (batch) 0 (loss) 0.1863\n",
      "(epoch) 12 (batch) 250 (loss) 0.2436\n",
      "(epoch) 12 (batch) 500 (loss) 0.2417\n",
      "(epoch) 12 (batch) 750 (loss) 0.2332\n",
      "(epoch) 12 (batch) 1000 (loss) 0.2274\n",
      "Epoch 012 | Validation score: 0.8880 | Test score: 0.8877\n",
      "(epoch) 13 (batch) 0 (loss) 0.1798\n",
      "(epoch) 13 (batch) 250 (loss) 0.2205\n",
      "(epoch) 13 (batch) 500 (loss) 0.2500\n",
      "(epoch) 13 (batch) 750 (loss) 0.2314\n",
      "(epoch) 13 (batch) 1000 (loss) 0.2196\n",
      "Epoch 013 | Validation score: 0.8904 | Test score: 0.8902\n",
      "(epoch) 14 (batch) 0 (loss) 0.1716\n",
      "(epoch) 14 (batch) 250 (loss) 0.2216\n",
      "(epoch) 14 (batch) 500 (loss) 0.2385\n",
      "(epoch) 14 (batch) 750 (loss) 0.2332\n",
      "(epoch) 14 (batch) 1000 (loss) 0.2086\n",
      "Epoch 014 | Validation score: 0.8880 | Test score: 0.8876\n",
      "(epoch) 15 (batch) 0 (loss) 0.1738\n",
      "(epoch) 15 (batch) 250 (loss) 0.2223\n",
      "(epoch) 15 (batch) 500 (loss) 0.2281\n",
      "(epoch) 15 (batch) 750 (loss) 0.2318\n",
      "(epoch) 15 (batch) 1000 (loss) 0.2191\n",
      "Epoch 015 | Validation score: 0.8872 | Test score: 0.8873\n",
      "(epoch) 16 (batch) 0 (loss) 0.1584\n",
      "(epoch) 16 (batch) 250 (loss) 0.2101\n",
      "(epoch) 16 (batch) 500 (loss) 0.2225\n",
      "(epoch) 16 (batch) 750 (loss) 0.2383\n",
      "(epoch) 16 (batch) 1000 (loss) 0.2272\n",
      "Epoch 016 | Validation score: 0.8786 | Test score: 0.8774\n",
      "(epoch) 17 (batch) 0 (loss) 0.1670\n",
      "(epoch) 17 (batch) 250 (loss) 0.2327\n",
      "(epoch) 17 (batch) 500 (loss) 0.2302\n",
      "(epoch) 17 (batch) 750 (loss) 0.2200\n",
      "(epoch) 17 (batch) 1000 (loss) 0.2293\n",
      "Epoch 017 | Validation score: 0.8884 | Test score: 0.8874\n",
      "(epoch) 18 (batch) 0 (loss) 0.1590\n",
      "(epoch) 18 (batch) 250 (loss) 0.2221\n",
      "(epoch) 18 (batch) 500 (loss) 0.2242\n",
      "(epoch) 18 (batch) 750 (loss) 0.2337\n",
      "(epoch) 18 (batch) 1000 (loss) 0.2341\n",
      "Epoch 018 | Validation score: 0.8801 | Test score: 0.8790\n",
      "(epoch) 19 (batch) 0 (loss) 0.1624\n",
      "(epoch) 19 (batch) 250 (loss) 0.2197\n",
      "(epoch) 19 (batch) 500 (loss) 0.2281\n",
      "(epoch) 19 (batch) 750 (loss) 0.2228\n",
      "(epoch) 19 (batch) 1000 (loss) 0.2271\n",
      "Epoch 019 | Validation score: 0.8841 | Test score: 0.8838\n",
      "(epoch) 20 (batch) 0 (loss) 0.1644\n",
      "(epoch) 20 (batch) 250 (loss) 0.2185\n",
      "(epoch) 20 (batch) 500 (loss) 0.2137\n",
      "(epoch) 20 (batch) 750 (loss) 0.2170\n",
      "(epoch) 20 (batch) 1000 (loss) 0.2211\n",
      "Epoch 020 | Validation score: 0.8851 | Test score: 0.8844\n",
      "(epoch) 21 (batch) 0 (loss) 0.1738\n",
      "(epoch) 21 (batch) 250 (loss) 0.2058\n",
      "(epoch) 21 (batch) 500 (loss) 0.2190\n",
      "(epoch) 21 (batch) 750 (loss) 0.1887\n",
      "(epoch) 21 (batch) 1000 (loss) 0.2163\n",
      "Epoch 021 | Validation score: 0.8831 | Test score: 0.8820\n",
      "(epoch) 22 (batch) 0 (loss) 0.1656\n",
      "(epoch) 22 (batch) 250 (loss) 0.1861\n",
      "(epoch) 22 (batch) 500 (loss) 0.2085\n",
      "(epoch) 22 (batch) 750 (loss) 0.1969\n",
      "(epoch) 22 (batch) 1000 (loss) 0.2092\n",
      "Epoch 022 | Validation score: 0.8787 | Test score: 0.8777\n",
      "(epoch) 23 (batch) 0 (loss) 0.1546\n",
      "(epoch) 23 (batch) 250 (loss) 0.1912\n",
      "(epoch) 23 (batch) 500 (loss) 0.2066\n",
      "(epoch) 23 (batch) 750 (loss) 0.1943\n",
      "(epoch) 23 (batch) 1000 (loss) 0.1947\n",
      "Epoch 023 | Validation score: 0.8744 | Test score: 0.8728\n",
      "(epoch) 24 (batch) 0 (loss) 0.1463\n",
      "(epoch) 24 (batch) 250 (loss) 0.2045\n",
      "(epoch) 24 (batch) 500 (loss) 0.1921\n",
      "(epoch) 24 (batch) 750 (loss) 0.1782\n",
      "(epoch) 24 (batch) 1000 (loss) 0.1973\n",
      "Epoch 024 | Validation score: 0.8790 | Test score: 0.8773\n",
      "(epoch) 25 (batch) 0 (loss) 0.1405\n",
      "(epoch) 25 (batch) 250 (loss) 0.2004\n",
      "(epoch) 25 (batch) 500 (loss) 0.1966\n",
      "(epoch) 25 (batch) 750 (loss) 0.1821\n",
      "(epoch) 25 (batch) 1000 (loss) 0.1934\n",
      "Epoch 025 | Validation score: 0.8798 | Test score: 0.8786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "report_frequency = len(X['train']) // batch_size // 5\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for iteration, batch_idx in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x_batch = X['train'][batch_idx]\n",
    "        y_batch = y['train'][batch_idx]\n",
    "        loss = loss_fn(apply_model(x_batch).squeeze(1), y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % report_frequency == 0:\n",
    "            print(f'(epoch) {epoch} (batch) {iteration} (loss) {loss.item():.4f}')\n",
    "\n",
    "    val_score = evaluate('val')\n",
    "    test_score = evaluate('test')\n",
    "    print(f'Epoch {epoch:03d} | Validation score: {val_score:.4f} | Test score: {test_score:.4f}', end='')\n",
    "    progress.update((-1 if task_type == 'regression' else 1) * val_score)\n",
    "    if progress.success:\n",
    "        print(' <<< BEST VALIDATION EPOCH', end='')\n",
    "        torch.save(model, \"models/rtdl_resnet_epsilon.pth\")\n",
    "    print()\n",
    "    if progress.fail:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19be3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score after training: 0.8936\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"models/rtdl_mlp_epsilon.pth\")\n",
    "print(f'Test score after training: {evaluate(\"test\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d148082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../attacks/\")\n",
    "import mister_ed.utils.pytorch_utils as utils\n",
    "from utils import load_image, torch_to_image, get_expl, convert_relu_to_softplus, plot_overview, UniGrad\n",
    "import captum\n",
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "16741bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6143],\n",
       "        [ 1.6499]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[\"test\"][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04a36666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[\"test\"][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "d63e1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sal = Saliency(model)\n",
    "# expl = sal.attribute(X[\"test\"][0:64])\n",
    "# int_grad = IntegratedGradients(model)\n",
    "# expl = int_grad.attribute(X[\"test\"][0:64])\n",
    "# inputxgrad = InputXGradient(model)\n",
    "# expl = inputxgrad.attribute(X[\"test\"][0:64])\n",
    "deeplift = DeepLift(model)\n",
    "expl = deeplift.attribute(X[\"test\"][0:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "c61dcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = torch.abs(expl)\n",
    "expl = expl / torch.sum(expl , dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "5c5d66fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2000])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "10b21fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 10\n",
    "topk_inds = torch.topk(expl, k=topk, dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "92ba0633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2733333333333334"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints = []\n",
    "for _ in range(30):\n",
    "    e1, e2=np.random.randint(64, size=2)\n",
    "#     print(e1, e2)\n",
    "    ints.append(float(len(np.intersect1d(topk_inds[e1].cpu().detach().numpy(),\n",
    "                            topk_inds[e2].cpu().detach().numpy())))/topk)\n",
    "np.mean(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f1f885d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_input = (torch.zeros((1, 2000))).to(device)\n",
    "expl_random = sal.attribute(rand_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "50a284f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n"
     ]
    }
   ],
   "source": [
    "print(float(len(np.intersect1d(topk_inds[e1].cpu().detach().numpy(),\n",
    "                            torch.topk(expl_random, k=topk, dim=1)[1].cpu().detach().numpy())))/topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "754844e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[\"test\"][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e55ab06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1794,  690,  784,  ..., 1478, 1073, 1201],\n",
       "        [1794, 1866,  484,  ..., 1296, 1029, 1678],\n",
       "        [1794,  789, 1062,  ..., 1577,  723, 1976],\n",
       "        ...,\n",
       "        [1315, 1481,   62,  ...,  507,  824, 1800],\n",
       "        [1697, 1794, 1062,  ...,   68,  308, 1759],\n",
       "        [1794,  484, 1187,  ...,  972,  767,  784]], device='cuda:0')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_inds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c9ce333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_L0_box_torch(y, k, lb, ub):\n",
    "    x = torch.clone(y)\n",
    "    p1 = torch.sum(x**2, dim=-1)\n",
    "    p2 = torch.minimum(torch.minimum(ub - x, x - lb), torch.zeros_like(x))\n",
    "    p2 = torch.sum(p2**2, dim=-1)\n",
    "    p3 = torch.sort(torch.reshape(p1-p2, (p2.size()[0],-1)))[0][:,-k]\n",
    "    x = x*(torch.logical_and(lb <=x, x <= ub)) + lb*(lb > x) + ub*(x > ub)\n",
    "    x = x * torch.unsqueeze((p1 - p2) >= p3.reshape([-1, 1, 1]), -1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d46e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_x = project_L0_box_torch(X[\"test\"][0:10].unsqueeze(dim=2).unsqueeze(dim=3), k=20, \n",
    "                     lb=torch.min(X[\"test\"], dim=0)[0].unsqueeze(0).unsqueeze(dim=2).unsqueeze(dim=3), \n",
    "                     ub=torch.max(X[\"test\"], dim=0)[0].unsqueeze(0).unsqueeze(dim=2).unsqueeze(dim=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd694201",
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_x = prj_x.squeeze(dim=3).squeeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb27abda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1146, 0.0930, 0.0964,  ..., 0.0778, 0.1156, 0.0644], device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(X[\"test\"], dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3b05275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1660],\n",
       "        [0.8389],\n",
       "        [0.9501],\n",
       "        [0.0032],\n",
       "        [0.6880]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(model(X[\"test\"][0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1f5816cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1446, device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"test\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "461c9c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  2.0000, -4.0000, -5.0000],\n",
       "        [-0.7000,  2.0000,  1.0000, -8.0000],\n",
       "        [-1.0000,  2.0000,  0.0000,  4.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,-4,-5],\n",
    "                  [-0.7, 2, 1, -8],\n",
    "                  [-1, 2, 0, 4]], device=\"cuda\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "24e2d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_coord(array, used_inds, k=1):\n",
    "    array_abs = torch.abs(array)\n",
    "    if len(used_inds)>0:\n",
    "        array_abs[np.array(used_inds).T.tolist()] = 0.0\n",
    "    inds = torch.topk(array_abs.view((array.size()[0],-1)), k=k, dim=1)[1].detach().cpu().numpy()\n",
    "    for k_ind in range(k):\n",
    "        chosen_inds = [[i, j[k_ind]] for i,j in enumerate(inds)]\n",
    "    ###\n",
    "    new_inds = np.array([[i, j[0]] for i,j in enumerate(inds)])\n",
    "    for k_ind in range(1,k):\n",
    "        new_inds = np.concatenate((new_inds, np.array([[i, j[k_ind]] for i,j in enumerate(inds)])))\n",
    "    ###\n",
    "    new_inds = new_inds.T.tolist()\n",
    "    ###\n",
    "    new_array = torch.zeros_like(array)\n",
    "    new_array[new_inds] = array[new_inds]\n",
    "    ###\n",
    "    return new_array, chosen_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "6e31810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2, a_inds = topk_coord(a, [[1,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0b2ba18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[np.array(a_inds).T.tolist()]\n",
    "c = (b<-1) | (b>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d29f0ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3], [2, 3]]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a_inds)[c.detach().cpu().numpy()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "d4fe44eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a_inds).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "71a60b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.5>=3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff171546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_robust_expls]",
   "language": "python",
   "name": "conda-env-pytorch_robust_expls-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
