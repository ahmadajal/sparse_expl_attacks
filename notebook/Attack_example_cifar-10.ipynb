{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../src/\")\n",
    "import numpy as np\n",
    "import scipy.spatial as spatial\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from models.resnet import ResNet18 as ResNet18_ReLu\n",
    "from models.resnet_softplus_10 import ResNet18\n",
    "from sparse_expl_attacks import image_datasets_dir, models_weights_dir, output_dir\n",
    "from sparse_expl_attacks.sparse_attack import SparseAttack\n",
    "from sparse_expl_attacks.utils import DifferentiableNormalize, get_expl, topk_intersect\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack params\n",
    "num_iter = 100\n",
    "lr = 0.1\n",
    "expl_method = \"saliency\"\n",
    "topk = 20\n",
    "max_num_features = 20\n",
    "gamma = 1e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (cpu or gpu).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 72\n",
    "# Set the seed.\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "# Defining the normalizer for CIFAR-10 images.\n",
    "data_mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "data_std = np.array([0.2023, 0.1994, 0.2010])\n",
    "normalizer = DifferentiableNormalize(mean=data_mean, std=data_std)\n",
    "# Loading the CIFAR-10 test dataset.\n",
    "cifar_test = torchvision.datasets.CIFAR10(\n",
    "    root=os.path.join(image_datasets_dir, \"cifar-10\"),\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=128, shuffle=True)\n",
    "BATCH_SIZE = 16\n",
    "indices = np.random.randint(128, size=BATCH_SIZE)\n",
    "# Load images.\n",
    "dataiter = iter(test_loader)\n",
    "images_batch, labels_batch = next(dataiter)\n",
    "examples = images_batch[indices].to(device)\n",
    "labels = labels_batch[indices].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc58abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample image\n",
    "# Convert torch tensor to image\n",
    "img = examples[0:1].permute(0, 2, 3, 1)\n",
    "img = img.contiguous().squeeze().detach().cpu().numpy()\n",
    "img = np.clip(img, 0, 1)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8530ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softplus model\n",
    "model = ResNet18()\n",
    "model.load_state_dict(torch.load(os.path.join(models_weights_dir, \"RN18_standard.pth\"))[\"net\"])\n",
    "model = model.eval().to(device)\n",
    "# ReLu model\n",
    "model_relu = ResNet18_ReLu()\n",
    "model_relu.load_state_dict(\n",
    "    torch.load(os.path.join(models_weights_dir, \"RN18_standard.pth\"))[\"net\"]\n",
    ")\n",
    "model_relu = model_relu.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only instances for which the model prediction is correct.\n",
    "with torch.no_grad():\n",
    "    preds = model_relu(normalizer.forward(examples)).argmax(dim=1).detach()\n",
    "    samples_to_pick = preds == labels\n",
    "    examples = examples[samples_to_pick]\n",
    "    labels = labels[samples_to_pick]\n",
    "BATCH_SIZE = examples.size()[0]\n",
    "print(f\"number of samples: {examples.size()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the attack.\n",
    "sparse_attack = SparseAttack(\n",
    "    model=model,\n",
    "    model_relu=model_relu,\n",
    "    expl_method=expl_method,\n",
    "    num_iter=num_iter,\n",
    "    lr=lr,\n",
    "    topk=topk,\n",
    "    max_num_features=max_num_features,\n",
    "    gamma=gamma,\n",
    "    normalizer=normalizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv_single_step = sparse_attack.attack(\n",
    "    attack_type=\"single_step\", x_input=examples, y_input=labels\n",
    ")\n",
    "\n",
    "x_adv_pgd0 = sparse_attack.attack(\n",
    "    attack_type=\"pgd0\", x_input=examples, y_input=labels\n",
    ")\n",
    "\n",
    "x_adv_increase_decrease = sparse_attack.attack(\n",
    "    attack_type=\"increase_decrease\", x_input=examples, y_input=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the explanation of a sample image for vizualising the result.\n",
    "b = 0\n",
    "org_expl = get_expl(\n",
    "    model, \n",
    "    normalizer.forward(examples[b:b+1]), \n",
    "    expl_method, \n",
    "    device,\n",
    "    labels[b:b+1], \n",
    "    normalize=False)\n",
    "\n",
    "adv_expl_single_step = get_expl(\n",
    "    model, \n",
    "    normalizer.forward(x_adv_single_step[b:b+1]), \n",
    "    expl_method, \n",
    "    device,\n",
    "    labels[b:b+1], \n",
    "    normalize=False)\n",
    "\n",
    "adv_expl_pgd0 = get_expl(\n",
    "    model, \n",
    "    normalizer.forward(x_adv_pgd0[b:b+1]), \n",
    "    expl_method, \n",
    "    device,\n",
    "    labels[b:b+1], \n",
    "    normalize=False)\n",
    "\n",
    "adv_expl_increase_decrease = get_expl(\n",
    "    model, \n",
    "    normalizer.forward(x_adv_increase_decrease[b:b+1]), \n",
    "    expl_method, \n",
    "    device,\n",
    "    labels[b:b+1], \n",
    "    normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4, figsize=(12, 6))\n",
    "ax[0,0].imshow(examples[b:b+1].squeeze().permute(1,2,0).detach().cpu().numpy())\n",
    "ax[0,0].set_title(\"Original image \\n(dog)\", fontsize=12)\n",
    "ax[0,0].axis(\"off\")\n",
    "viz.visualize_image_attr(\n",
    "    np.transpose(org_expl.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    np.transpose(examples[b:b+1].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    method='heat_map',\n",
    "    cmap=\"Reds\",\n",
    "    show_colorbar=False,\n",
    "    outlier_perc=2,\n",
    "    fig_size=(4,4), plt_fig_axis=(fig, ax[1,0]), use_pyplot=False)\n",
    "ax[1,0].axis(\"off\")\n",
    "ax[1,0].set_title(\"Original explanation\", fontsize=12)\n",
    "\n",
    "ax[0,1].imshow(x_adv_single_step[b:b+1].squeeze().permute(1,2,0).detach().cpu().numpy())\n",
    "ax[0,1].axis(\"off\")\n",
    "ax[0,1].set_title(\"Perturbed image (Single-step)\", fontsize=12)\n",
    "viz.visualize_image_attr(\n",
    "    np.transpose(adv_expl_single_step.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    np.transpose(x_adv_single_step[b:b+1].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    method='heat_map',\n",
    "    cmap=\"Reds\",\n",
    "    show_colorbar=False,\n",
    "    outlier_perc=2,\n",
    "    fig_size=(4,4), plt_fig_axis=(fig, ax[1,1]), use_pyplot=False)\n",
    "ax[1,1].axis(\"off\")\n",
    "ax[1,1].set_title(\"Perturbed explanation\", fontsize=12)\n",
    "\n",
    "ax[0,2].imshow(x_adv_pgd0[b:b+1].squeeze().permute(1,2,0).detach().cpu().numpy())\n",
    "ax[0,2].axis(\"off\")\n",
    "ax[0,2].set_title(\"Perturbed image (PGD with $\\ell_0$)\", fontsize=12)\n",
    "viz.visualize_image_attr(\n",
    "    np.transpose(adv_expl_pgd0.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    np.transpose(x_adv_pgd0[b:b+1].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    method='heat_map',\n",
    "    cmap=\"Reds\",\n",
    "    show_colorbar=False,\n",
    "    outlier_perc=2,\n",
    "    fig_size=(4,4), plt_fig_axis=(fig, ax[1,2]), use_pyplot=False)\n",
    "ax[1,2].axis(\"off\")\n",
    "ax[1,2].set_title(\"Perturbed explanation\", fontsize=12)\n",
    "\n",
    "ax[0,3].imshow(x_adv_increase_decrease[b:b+1].squeeze().permute(1,2,0).detach().cpu().numpy())\n",
    "ax[0,3].axis(\"off\")\n",
    "ax[0,3].set_title(\"Perturbed image (GrID)\", fontsize=12)\n",
    "viz.visualize_image_attr(\n",
    "    np.transpose(adv_expl_increase_decrease.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    np.transpose(x_adv_increase_decrease[b:b+1].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "    method='heat_map',\n",
    "    cmap=\"Reds\",\n",
    "    show_colorbar=False,\n",
    "    outlier_perc=2,\n",
    "    fig_size=(4,4), plt_fig_axis=(fig, ax[1,3]), use_pyplot=False)\n",
    "ax[1,3].axis(\"off\")\n",
    "ax[1,3].set_title(\"Perturbed explanation\", fontsize=12)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_robust_expls]",
   "language": "python",
   "name": "conda-env-pytorch_robust_expls-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
